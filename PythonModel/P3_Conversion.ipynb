{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78a61604",
   "metadata": {},
   "source": [
    "# Discriminación Gamma/Neutrón basada en ML\n",
    "\n",
    "Flujo de trabajo basado en R. S. Molina, I. R. Morales, M. L. Crespo, V. G. Costa, S. Carrato y G. Ramponi, \"An End-to-End Workflow to Efficiently Compress and Deploy DNN Classifiers on SoC/FPGA\", en IEEE Embedded Systems Letters, vol. 16, no. 3, pp. 255-258, septiembre 2024, doi: 10.1109/LES.2023.3343030.\n",
    "\n",
    "Código adaptado del repositorio oficial de \"An End-to-End Workflow to Efficiently Compress and Deploy DNN Classifiers on SoC/FPGA\".\n",
    "\n",
    "Uso de dataset abierto disponible en: https://doi.org/10.5281/zenodo.8037058\n",
    "\n",
    "Documentación de hls4ml: https://fastmachinelearning.org/hls4ml/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f3cca",
   "metadata": {},
   "source": [
    "--- \n",
    "### Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6839b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from qkeras import *\n",
    "from qkeras import QActivation\n",
    "from qkeras import QDense, QConv2DBatchnorm\n",
    "\n",
    "import hls4ml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d0d59a",
   "metadata": {},
   "source": [
    "# Ruta hacia Vitis HLS\n",
    "\n",
    "El directorio de Vitis HLS debe especificarse para poder usar `hls4ml` junto con la herramienta de high-level synthesis (síntesis de alto nivel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77af0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH Xilinx Virtual Machine\n",
    "os.environ['PATH'] = '/mnt/d/Xilinx/Vitis_HLS/2022.2/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = '/mnt/d/Xilinx/Vitis/2022.2/bin:' + os.environ['PATH']\n",
    "os.environ['PATH'] = '/mnt/d/Xilinx/Vivado/2022.2/bin:' + os.environ['PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0e9fb7",
   "metadata": {},
   "source": [
    "## Cargar el modelo\n",
    "\n",
    "Se carga el modelo del estudiante para hacer la conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1177e26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"studentMLP\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " fc1 (QDense)                (None, 6)                 972       \n",
      "                                                                 \n",
      " relu0 (QActivation)         (None, 6)                 0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 6)                 0         \n",
      "                                                                 \n",
      " fc2 (QDense)                (None, 4)                 28        \n",
      "                                                                 \n",
      " relu1 (QActivation)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 4)                 0         \n",
      "                                                                 \n",
      " fc3 (QDense)                (None, 2)                 10        \n",
      "                                                                 \n",
      " relu2 (QActivation)         (None, 2)                 0         \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 2)                 0         \n",
      "                                                                 \n",
      " fc4 (QDense)                (None, 4)                 12        \n",
      "                                                                 \n",
      " relu3 (QActivation)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 4)                 0         \n",
      "                                                                 \n",
      " fc5 (QDense)                (None, 3)                 15        \n",
      "                                                                 \n",
      " relu4 (QActivation)         (None, 3)                 0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 3)                 0         \n",
      "                                                                 \n",
      " output (QDense)             (None, 2)                 8         \n",
      "                                                                 \n",
      " outputActivation (Activatio  (None, 2)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,045\n",
      "Trainable params: 1,045\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load keras model from file\n",
    "\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "\n",
    "co = {}\n",
    "_add_supported_quantized_objects(co)\n",
    "\n",
    "model = load_model('./models/studentModel_GN_GICM.h5', custom_objects=co)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b45bdc8",
   "metadata": {},
   "source": [
    "### Integracion con hls4ml\n",
    "\n",
    "hls4ml es un paquete de Python desarrollado para convertir modelos de machine learning (**ML**) en proyectos de HLS (**High-Level Synthesis**), lo que permite desplegar inferencias basadas en ML en hardware como **FPGAs**. Más detalles se pueden encontrar en la documentación de hls4ml.\n",
    "\n",
    "El usuario puede controlar varias opciones relacionadas con el modelo, incluyendo:\n",
    "\n",
    "- **Precisión**: Permite definir la precisión de los cálculos en el modelo, por ejemplo, usando representación de punto fijo (fixed-point) o punto flotante (floating-point).\n",
    "\n",
    "- **Flujo de datos / Reutilización de recursos**: Controla el nivel de paralelismo o streaming en la implementación del modelo, permitiendo distintos grados de pipelining.\n",
    "\n",
    "- **Quantization Aware Training (QAT)**: Permite lograr un rendimiento optimizado con baja precisión utilizando herramientas como **QKeras**. Los modelos entrenados con QKeras se benefician automáticamente del análisis que hace **hls4ml** de los modelos **QKeras** durante la inferencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06bb31c",
   "metadata": {},
   "source": [
    "Para implementar un modelo en FPGA, se debe crear una configuración HLS usando la función:\n",
    "\n",
    "`hls4ml.utils.config_from_keras_model(kerasModel, granularity)`\n",
    "\n",
    "donde:\n",
    "\n",
    "kerasModel es el modelo preentrenado que se quiere implementar en FPGA.\n",
    "\n",
    "granularity determina el nivel de configuración, y puede tomar dos valores:\n",
    "\n",
    "'model': La misma configuración se aplica a todo el modelo (por ejemplo, todas las capas usan precisión de 16 bits en punto fijo).\n",
    "\n",
    "'name': Se pueden aplicar configuraciones específicas por capa (por ejemplo, la capa de entrada en 8 bits punto fijo, mientras que la segunda capa en 16 bits punto fijo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbd9183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: inputLayer, layer type: InputLayer, input shapes: [[None, 161]], output shape: [None, 161]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 161]], output shape: [None, 6]\n",
      "Layer name: relu0, layer type: Activation, input shapes: [[None, 6]], output shape: [None, 6]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 6]], output shape: [None, 4]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 4]], output shape: [None, 4]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 4]], output shape: [None, 2]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 2]], output shape: [None, 2]\n",
      "Layer name: fc4, layer type: QDense, input shapes: [[None, 2]], output shape: [None, 4]\n",
      "Layer name: relu3, layer type: Activation, input shapes: [[None, 4]], output shape: [None, 4]\n",
      "Layer name: fc5, layer type: QDense, input shapes: [[None, 4]], output shape: [None, 3]\n",
      "Layer name: relu4, layer type: Activation, input shapes: [[None, 3]], output shape: [None, 3]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 3]], output shape: [None, 2]\n",
      "Layer name: outputActivation, layer type: Activation, input shapes: [[None, 2]], output shape: [None, 2]\n"
     ]
    }
   ],
   "source": [
    "hls_config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab505804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Model\n",
      "  Precision\n",
      "    default:         fixed<16,6>\n",
      "  ReuseFactor:       1\n",
      "  Strategy:          Latency\n",
      "  BramFactor:        1000000000\n",
      "  TraceOutput:       False\n",
      "LayerName\n",
      "  inputLayer\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "  fc1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      weight:        fixed<8,5,TRN,WRAP,0>\n",
      "      bias:          fixed<8,5,TRN,WRAP,0>\n",
      "  fc1_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "  relu0\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<8,1,RND_CONV,SAT,0>\n",
      "  fc2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      weight:        fixed<8,5,TRN,WRAP,0>\n",
      "      bias:          fixed<8,5,TRN,WRAP,0>\n",
      "  fc2_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "  relu1\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<8,1,RND_CONV,SAT,0>\n",
      "  fc3\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      weight:        fixed<8,5,TRN,WRAP,0>\n",
      "      bias:          fixed<8,5,TRN,WRAP,0>\n",
      "  fc3_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "  relu2\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<8,1,RND_CONV,SAT,0>\n",
      "  fc4\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      weight:        fixed<8,5,TRN,WRAP,0>\n",
      "      bias:          fixed<8,5,TRN,WRAP,0>\n",
      "  fc4_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "  relu3\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<8,1,RND_CONV,SAT,0>\n",
      "  fc5\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      weight:        fixed<8,5,TRN,WRAP,0>\n",
      "      bias:          fixed<8,5,TRN,WRAP,0>\n",
      "  fc5_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "  relu4\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        fixed<8,1,RND_CONV,SAT,0>\n",
      "  output\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "      weight:        fixed<16,7,TRN,WRAP,0>\n",
      "      bias:          fixed<16,7,TRN,WRAP,0>\n",
      "  output_linear\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "  outputActivation\n",
      "    Trace:           False\n",
      "    Precision\n",
      "      result:        auto\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "from src import plotting\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(hls_config)\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96da46b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for Layer in hls_config['LayerName'].keys():\n",
    "    hls_config['LayerName'][Layer]['Strategy'] = 'Latency'\n",
    "    hls_config['LayerName'][Layer]['ReuseFactor'] = 1\n",
    "    hls_config['LayerName'][Layer]['Precision'] = 'ap_fixed<8,4>'\n",
    "\n",
    "hls_config['LayerName']['outputActivation']['Strategy'] = 'Stable'\n",
    "hls_config['Model']['Precision'] = 'ap_fixed<16,6>'\n",
    "\n",
    "hls_config['LayerName']['inputLayer']['Precision'] = 'ap_fixed<16,6>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b72ae66",
   "metadata": {},
   "source": [
    "###  hls4ml con Vitis HLS como backend\n",
    "\n",
    "---\n",
    "\n",
    "Se debe correr la celda, saldra un error, pero al final es necesario ejecutar en la terminal lo siguiente:\n",
    "\n",
    "\n",
    "``` bash\n",
    "$env:PATH += \";D:\\Xilinx\\Vitis_HLS\\2022.2\\bin\"\n",
    "vitis_hls -version\n",
    "cd .\\pythonModel\\hlsPrj\\\n",
    "vitis_hls -f .\\build_prj.tcl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7cccaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: inputLayer, layer type: InputLayer, input shapes: [[None, 161]], output shape: [None, 161]\n",
      "Layer name: fc1, layer type: QDense, input shapes: [[None, 161]], output shape: [None, 6]\n",
      "Layer name: relu0, layer type: Activation, input shapes: [[None, 6]], output shape: [None, 6]\n",
      "Layer name: fc2, layer type: QDense, input shapes: [[None, 6]], output shape: [None, 4]\n",
      "Layer name: relu1, layer type: Activation, input shapes: [[None, 4]], output shape: [None, 4]\n",
      "Layer name: fc3, layer type: QDense, input shapes: [[None, 4]], output shape: [None, 2]\n",
      "Layer name: relu2, layer type: Activation, input shapes: [[None, 2]], output shape: [None, 2]\n",
      "Layer name: fc4, layer type: QDense, input shapes: [[None, 2]], output shape: [None, 4]\n",
      "Layer name: relu3, layer type: Activation, input shapes: [[None, 4]], output shape: [None, 4]\n",
      "Layer name: fc5, layer type: QDense, input shapes: [[None, 4]], output shape: [None, 3]\n",
      "Layer name: relu4, layer type: Activation, input shapes: [[None, 3]], output shape: [None, 3]\n",
      "Layer name: output, layer type: QDense, input shapes: [[None, 3]], output shape: [None, 2]\n",
      "Layer name: outputActivation, layer type: Activation, input shapes: [[None, 2]], output shape: [None, 2]\n",
      "Creating HLS model\n",
      "Writing HLS project\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "\".\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n",
      "\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Failed to compile project \"myproject\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 15\u001b[0m\n\u001b[0;32m     11\u001b[0m cfg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPart\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxc7z020clg484-1\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# PYNQ-Z1 or Zedboard: xc7z020clg484-1  \u001b[39;00m\n\u001b[0;32m     13\u001b[0m hls_model \u001b[38;5;241m=\u001b[39m hls4ml\u001b[38;5;241m.\u001b[39mconverters\u001b[38;5;241m.\u001b[39mkeras_to_hls(cfg)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mhls_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m### Usar en terminal el sigueinte comando\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# $env:PATH += \";D:\\Xilinx\\Vitis_HLS\\2022.2\\bin\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# vitis_hls -version\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# cd .\\pythonModel\\hlsPrj\\\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# vitis_hls -f .\\build_prj.tcl\u001b[39;00m\n",
      "File \u001b[1;32mc:\\GitHub\\FPGA_ml4hls_prj\\PythonModel\\env\\lib\\site-packages\\hls4ml\\model\\graph.py:695\u001b[0m, in \u001b[0;36mModelGraph.compile\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compile the generated project and link the library into current environment.\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \n\u001b[0;32m    692\u001b[0m \u001b[38;5;124;03mUsers should call this function if they want to use `predict` functionality for simulation.\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite()\n\u001b[1;32m--> 695\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\GitHub\\FPGA_ml4hls_prj\\PythonModel\\env\\lib\\site-packages\\hls4ml\\model\\graph.py:698\u001b[0m, in \u001b[0;36mModelGraph._compile\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compile\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 698\u001b[0m     lib_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_top_function_lib \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m platform\u001b[38;5;241m.\u001b[39msystem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLinux\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\GitHub\\FPGA_ml4hls_prj\\PythonModel\\env\\lib\\site-packages\\hls4ml\\backends\\fpga\\fpga_backend.py:174\u001b[0m, in \u001b[0;36mFPGABackend.compile\u001b[1;34m(self, model)\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret_val\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mprint\u001b[39m(ret_val\u001b[38;5;241m.\u001b[39mstdout)\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFailed to compile project \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_project_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    175\u001b[0m lib_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/firmware/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.so\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    176\u001b[0m     model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_output_dir(), model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_project_name(), model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget_config_value(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStamp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    177\u001b[0m )\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lib_name\n",
      "\u001b[1;31mException\u001b[0m: Failed to compile project \"myproject\""
     ]
    }
   ],
   "source": [
    "# Create configuration for Vitis HLS as backend.\n",
    "cfg = hls4ml.converters.create_config(backend='Vitis')\n",
    "\n",
    "# HLSConfig correspond to the configuration created in hls_config \n",
    "cfg['HLSConfig']  = hls_config\n",
    "# Model to be converted\n",
    "cfg['KerasModel'] = model\n",
    "# Folder where the HLS project will be created\n",
    "cfg['OutputDir']  = './hlsPrj/'\n",
    "# FPGA part \n",
    "cfg['Part'] = 'xc7z020clg484-1'  # PYNQ-Z1 or Zedboard: xc7z020clg484-1  \n",
    "  \n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "\n",
    "hls_model.compile()\n",
    "### Usar en terminal el sigueinte comando\n",
    "# $env:PATH += \";D:\\Xilinx\\Vitis_HLS\\2022.2\\bin\"\n",
    "# vitis_hls -version\n",
    "# cd .\\pythonModel\\hlsPrj\\\n",
    "# vitis_hls -f .\\build_prj.tcl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1fece47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vivado synthesis report not found.\n",
      "Implementation report not found.\n",
      "Timing report not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'CSynthesisReport': {'TargetClockPeriod': '5.00',\n",
       "  'EstimatedClockPeriod': '3.554',\n",
       "  'BestLatency': '31',\n",
       "  'WorstLatency': '31',\n",
       "  'IntervalMin': '1',\n",
       "  'IntervalMax': '1',\n",
       "  'BRAM_18K': '1',\n",
       "  'DSP': '5',\n",
       "  'FF': '15288',\n",
       "  'LUT': '15380',\n",
       "  'URAM': '0',\n",
       "  'AvailableBRAM_18K': '280',\n",
       "  'AvailableDSP': '220',\n",
       "  'AvailableFF': '106400',\n",
       "  'AvailableLUT': '53200',\n",
       "  'AvailableURAM': '0'},\n",
       " 'CosimReport': {'RTL': 'Verilog',\n",
       "  'Status': 'Pass',\n",
       "  'LatencyMin': 31,\n",
       "  'LatencyMax': 31,\n",
       "  'IntervalMin': 1,\n",
       "  'IntervalMax': 1,\n",
       "  'LatencyAvg': 31.0,\n",
       "  'IntervalAvg': 1.0}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # This will perform the synthesis of the HLS project, showing the main results (latency and resource usage) once the process is completed. \n",
    "\n",
    "hls_model.build(csim=False, export=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
